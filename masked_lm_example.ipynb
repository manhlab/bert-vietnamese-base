{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1215 11:33:16.521343 4744474048 file_utils.py:33] TensorFlow version 2.0.0 available.\n",
      "I1215 11:33:16.522012 4744474048 file_utils.py:40] PyTorch version 1.3.0.post2 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "from transformers.modeling_bert import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1215 11:33:18.672933 4744474048 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-vocab.txt from cache at /Users/m-suzuki/.cache/torch/transformers/ae4d597c0697c617ab6c6c913effc265b7e606830a3b373dfcaeca1794ab9229.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1215 11:33:20.271379 4744474048 configuration_utils.py:160] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json from cache at /Users/m-suzuki/.cache/torch/transformers/3c7c1cf64fda50b267a0e88bd23a88a7eaae4e8205f6ee4ceb472ef9e3274f29.3d9f46e22d98b122f65c2c48727301903ca4a5381c3e967e2ba93f0ae64dee75\n",
      "I1215 11:33:20.272289 4744474048 configuration_utils.py:177] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "I1215 11:33:21.143578 4744474048 modeling_utils.py:401] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin from cache at /Users/m-suzuki/.cache/torch/transformers/a2f827453709c7a74b48468cfea4ca3c7ebc260308a4cd52d5231dd02b73e315.6225a45d8bbedb200842a8cef5bc97d0631dad427835b9e916dfeb1c3b2a91e8\n",
      "I1215 11:33:23.478094 4744474048 modeling_utils.py:476] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(f'''\n",
    "    青葉山で{tokenizer.mask_token}の研究をしています。\n",
    "''', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 14211, 28646,    12,     4,     5,   426,    11,    15,    16,\n",
      "            21,  2610,     8,     3]])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '青葉', '##山', 'で', '[MASK]', 'の', '研究', 'を', 'し', 'て', 'い', 'ます', '。', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(input_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "masked_index = torch.where(input_ids == tokenizer.mask_token_id)[1].tolist()[0]\n",
    "print(masked_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 青葉山 で 植物 の 研究 を し て い ます 。 [SEP]\n",
      "[CLS] 青葉山 で 温泉 の 研究 を し て い ます 。 [SEP]\n",
      "[CLS] 青葉山 で 化石 の 研究 を し て い ます 。 [SEP]\n",
      "[CLS] 青葉山 で 蝶 の 研究 を し て い ます 。 [SEP]\n",
      "[CLS] 青葉山 で 昆虫 の 研究 を し て い ます 。 [SEP]\n"
     ]
    }
   ],
   "source": [
    "result = model(input_ids)\n",
    "pred_ids = result[0][:, masked_index].topk(5).indices.tolist()[0]\n",
    "for pred_id in pred_ids:\n",
    "    output_ids = input_ids.tolist()[0]\n",
    "    output_ids[masked_index] = pred_id\n",
    "    print(tokenizer.decode(output_ids))\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
